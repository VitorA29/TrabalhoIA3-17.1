A metodologia abordada foi dividida em três partes. A primeira parte consiste na comparação da abordagem de redução de dimensionalidade PCA \cite{Jolliffe:2002} e seleção de atributos RFE \cite{Guyon2003} aplicados no classificador SVM \cite{MachineLearning:Michalski:2013}. A segunda parte trata da seleção de parâmetros utilizando a técnica de Grid Search \cite{SnoekLA12}. Por fim, a terceira parte confere a execução dos classificadores Naive Bayes, SVM, Decision Tree e Random Forest \cite{MachineLearning:Michalski:2013} em cima da base de dados selecionada.

A base de dados utilizada é formada por tweets sobre os produtos e serviços fornecidos pela Apple e fornecida pela Carnegie Mellon University\footnote{http://boston.lti.cs.cmu.edu/classes/95-865-K/HW/HW3/twitter-sanders-apple2.zip} \footnote{http://boston.lti.cs.cmu.edu/classes/95-865-K/HW/HW3/twitter-sanders-apple3.zip}. A base foi tratada segundo a abordagem de \textit{Bag of Words} utilizando a ferramenta NLTK\footnote{http://www.nltk.org/}. A seleção da base baseou-se na proximidade das informações nela presente com o conhecimento de mundo dos integrantes do grupo.

Os algoritmos supracitados e técnicas foram implementadas conforme a ferramenta de aprendizado de máquina SciKit-Learn\footnote{http://scikit-learn.org/stable/} e a linguagem utilizada foi Python\footnote{https://www.python.org/} \textit{release} 3.5.2.

\subsection{Métodos de Aprendizado de Máquina Usados}
\begin{enumerate}
	\item \textit{Naive Bayes}: O classficador Naïve Bayes é um classificador probabilistico baseado no teorema de Bayes com forte independência entre as \textit{features} \cite{Han2005}.
	\item \textit{SVM}: O classificador \textit{Support Vector Machines} são modelos de aprendizado de máquina supervisionado que criam modelos de associação através dos exemplos, onde esses são mapeados e é definido uma "linha" entre os conjuntos de dados. A partir desde modelo os novos dados são categorizados nos grupos existentes.
	\item \textit{Decision Tree}: O classificador Árvore de Decisão a partir de tuplas treinadas é uma das categorias de árvores de decisão. Essa árvore é uma árvore de estrutura similar a um fluxograma, onde cada nó não folha denota um teste em um atributo, cada ramo representa um resultado do teste e cada nó folha representa a classe rótulo \cite{Han2005}.
	\item \textit{Random Forest}: O classificador \textit{Random Forest} é um método \textit{ensemble}. Cada classificador existente no \textit{ensemble} é um classificador do tipo árvore de decisão e sua coleção é chamada de floresta. Por fim, as árvores de decisão individuais são geradas selecionado atributos aleatórios de cada nó \cite{Han2005}.
\end{enumerate}

\subsection{PCA \textit{vs.} RFE}
PCA é uma técnica que converte um conjunto de possíveis variáveis correlatas em um conjunto de variáveis não correlatas chamada de 'componentes principais', usando transformação ortogonal. Ela é uma técnica que visa encontrar as features mais importantes para a variação dos dados. É usado justamente para reduzir a dimensionalidade de um conjunto de dados muito grande. Em poucas palavras, o PCA pode ser representado pela seguinte pergunta: \textit{Existe algum subconjunto menor de parâmetros, 30\% por exemplo, que consegue explicar 70\% ou mais da variação do dado?}

\textit{Feature Selection}, também conhecido como \textit{Variable Selection} ou \textit{Attribute Selection}, é o processo de seleção, do conjunto de treinamento, de um subconjunto de features mais relevantes. Tem como objetivo facilitar o classificador, tornando-o mais eficiente, pois diminuirá o número de \textit{features}. E isso é muito importante para classificadores no qual o número de \textit{features} afeta no tempo de treinamento. E, em segundo, \textit{Feature Selection}, normalmente, aumenta a precisão, pois as \textit{features} que desviam do conjunto padrão e que podem causar uma piora na precisao são eliminadas.

\textit{Feature Selection} é diferente da Redução de Dimensionalidade. Ambos métodos tem como objetivo diminuir o número de \textit{features} da base de dados, mas a redução de dimensionalidade realiza esse trabalho combinando os atributos, enquanto a \textit{feature selection} inclue e excluem atributos presentes na base.

\subsection{\textit{Grid Search}}
O \textit{Grid Search} é o método tradicional para a otimização de hiperparametros que é a busca exaustiva por um subconjunto dos hiperparâmentros de um algortimo de aprendizado. Este método é necessário ser executado com auxílio de alguma métrica de performace que, normalmente, pode ser mensurado pela validação cruzada do conjunto de teste ou avaliação do conjunto de validação \cite{Hsu10}.