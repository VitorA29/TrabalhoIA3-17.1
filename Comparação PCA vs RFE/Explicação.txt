Tanto o PCA quanto o RFE reduzem o espaço de features(palavras no nosso caso) agrupando(PCA) ou eliminando(RFE). Por exemplo, suponha um universo onde as pessoas usam 'bom' e 'bacana' com exatamente a mesma semântica e expressividade. Essas duas palavras, dada essa situação, podem ser consideradas uma só, pois refletem o mesmo sentimento, ou seja, elas podem ser "agrupadas". Isso significa que o espaço vai diminuir, agora vou ter n-1 palavras pra me importar com o significado.

Por que, no binário, o PCA não deu diferença com 1000 como numero de features?
Porque o número de features é 479, não tem como "reduzir" pra 1000, então continua a mesma coisa.

Se o numero de features for 100 significa que ele vai reduzir o numero de features para 100. Ou seja, caso existam 5000 palavras, elas serão agrupadas ou eliminadas até que só restem 100.

O preço a se pagar pela redução do espaço é a diminuição da precisão.

Feature Selection(RFE) vs PCA(TruncatedSVD)
A diferença dos dois é que um AGRUPA e o outro EXCLUI.
o PCA agrupa, como descrito a cima, e o RFE elimina palavras inúteis.

RFE é lento, só rodou no binário.

---> LER OS LINKS <---
http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html
https://stats.stackexchange.com/questions/182711/principal-component-analysis-vs-feature-selection
https://www.quora.com/What-is-an-intuitive-explanation-for-PCA
http://setosa.io/ev/principal-component-analysis/
https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/